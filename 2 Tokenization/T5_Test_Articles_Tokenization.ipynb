{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "T5 Test Articles Tokenization.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j3FyCSf9sdIH"
      },
      "source": [
        "### This script tokenizes the articles in test set using T5 tokenizer from Huggingface. The tokenized articles were then truncated under several different truncation conditions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-RfzTCZCtW8n"
      },
      "source": [
        "!pip install transformers\n",
        "!pip install sentencepiece\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "40toBbDLtr9M"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from transformers import T5Tokenizer\n",
        "from time import time\n",
        "\n",
        "tokenizer = T5Tokenizer.from_pretrained(\"t5-base\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IVcyGUYEsivg"
      },
      "source": [
        "### Tokenize with T5 tokenizer, max length = 512"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "olOSxKkNGR99"
      },
      "source": [
        "# load the test dataset, tokenize the articles\n",
        "test_df = pd.read_csv('/content/drive/MyDrive/NLP Summarization Project/Data/Cleaned Test Data.csv')\n",
        "test_source = test_df['article'].tolist()\n",
        "\n",
        "articles_tokenized = tokenizer(test_source, max_length= None, truncation='only_first', padding='max_length',return_tensors='pt')\n",
        "input_array = articles_tokenized['input_ids'].numpy()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XXX-7uoBsqpc"
      },
      "source": [
        "# save tensors\n",
        "test_tensor = torch.tensor(input_array)\n",
        "torch.save(test_tensor, '/content/drive/My Drive/~cleandata/tensors/t5_512.pt')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_RiP4l8ZFuTg"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}