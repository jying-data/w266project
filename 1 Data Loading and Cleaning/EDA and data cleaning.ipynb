{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from time import time\n",
    "from collections import Counter\n",
    "import random\n",
    "\n",
    "df = pd.read_csv('newsroom_training_data_original.csv')\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# remove null article/summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6 entries have no summaries because the original json was \"null\", read into pandas as nan. Those are removed. 1 entry has no article."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(df[df['summary'].isna()]))\n",
    "df = df[df['summary'].notna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(df[df['article'].isna()]))\n",
    "df = df[df['article'].notna()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# remove foreign language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "foreign = df[df['language']!='en']\n",
    "row = random.randint(0, len(foreign)-1)\n",
    "print(foreign.iloc[row]['language'])\n",
    "print('------------------')\n",
    "print(foreign.iloc[row]['summary'])\n",
    "print('------------------')\n",
    "print(foreign.iloc[row]['article'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2200 Non-English articles removed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(df[df['language']!='en']))\n",
    "    \n",
    "df = df[df['language'] == 'en']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## clean up ascii character error\n",
    "\n",
    "When splitting summary by spaces, we observe some extreme outliers in terms of summary lengths. On the lower end, we saw 2411 summaries that have length of 1. Manually checking those, we see that some of the summaries were proper summaries but with ascii symbols improperly encoded. There were 190 examples affected by this, vast majority of them belonging to aol. We addressed this by manually replacing the ascii symbols."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ascii_replacer(col):\n",
    "    if '%20' not in col:\n",
    "        return col\n",
    "    \n",
    "    ascii_dict = {'%20': ' ', '%27': \"'\", '%2C': ',', '%0A': ' ', '%24': '$', '%3A': ':', '%25': 'percent',\n",
    "                  '%28': '(', '%29': ')', '%3F': '?', '%26': '&', '%3B': ';', \n",
    "                  '%u2019': \"'\", '%7C': '|', '%22': '\"', '%21': '!'}\n",
    "    for key, value in ascii_dict.items():\n",
    "        col = col.replace(key, value)\n",
    "    return col\n",
    "\n",
    "df['summary'] = df['summary'].apply(ascii_replacer)\n",
    "df['summary_length'] = df['summary'].apply(lambda x: len(x.split(' ')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# remove hyperlink"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2159 summaries contain a hyperlink. Manual inspection noted majority of these summaries were long and the url does not contribute information to the summary (often promotional links). These summaries do not match the type of summaries we aim to obtain, therefore they are excluded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperlinked = df[(df['summary'].str.contains('http')) | (df['summary'].str.contains('html')) |\\\n",
    "              (df['summary'].str.contains('www'))]\n",
    "row = random.randint(0, len(hyperlinked)-1)\n",
    "print(hyperlinked.iloc[row]['summary'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperlinked = df[(df['summary'].str.contains('http')) | (df['summary'].str.contains('html')) |\\\n",
    "              (df['summary'].str.contains('www'))]\n",
    "\n",
    "df = df[(~df['summary'].str.contains('http')) & (~df['summary'].str.contains('html')) &\\\n",
    "        (~df['summary'].str.contains('www'))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After previous removal, there are 2212 summaries with length of 1. Manual inspection shows they are largely nonsensical words and do not provide information to the articles. These were removed. We repeated this with summary length 2-5 as well, as they do not provide a reasonable summarization. A total 22553 summaries were removed. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# remove summary of 5 words or shorter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['summary_length'] = df['summary'].apply(lambda x: len(str(x).split()) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.percentile(df['summary_length'], [0.01, 0.05, 0.1, 0.2, 0.5, 1, 2, 3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_words = 6\n",
    "\n",
    "short = df[df['summary_length'] < num_words]\n",
    "row = random.randint(0, len(short)-1)\n",
    "print(short.iloc[row]['summary'])\n",
    "\n",
    "print(f\"summaries shorter than {num_words} words: {len(df[df['summary_length'] < num_words])}\\n\")\n",
    "\n",
    "# df = df[df['summary_length'] >= num_words]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# remove duplicate summaries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While manually examining the short summaries, we noticed duplicates such as \"Follow 2008 Elections & Campaigns at washingtonpost.com.\" that do not actually sum up the article. This is the result of poor data sanitiation during data scrapping by the provider of dataset. Therefore, we check for summaries with identical texts and remove those. This removes 16179 records."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('before deleting duplicate summaries,', len(df))\n",
    "\n",
    "summary_counts = pd.DataFrame(df['summary'].value_counts()).reset_index()\n",
    "summary_counts = summary_counts[summary_counts['summary'] != 1]\n",
    "duplicate_summary_list = summary_counts['index']\n",
    "\n",
    "df['summary_duplicate'] = df['summary'].isin(duplicate_summary_list)\n",
    "df = df[~df['summary_duplicate']]\n",
    "\n",
    "del df['summary_duplicate']\n",
    "\n",
    "print('after deleting duplicate summaries,', len(df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# remove duplicate articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "article_counts = pd.DataFrame(df['article'].value_counts()).reset_index()\n",
    "article_counts = article_counts[article_counts['article']>1]\n",
    "duplicate_article_list = article_counts['index']\n",
    "\n",
    "df['article_duplicate'] = df['article'].isin(duplicate_article_list)\n",
    "df = df[~df['article_duplicate']]\n",
    "\n",
    "del df['article_duplicate']\n",
    "\n",
    "print('after deleting duplicate articles,', len(df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On the higher end, the distribution of summary length has a long tail, with 1% having more than 183 words, and less than 0.2% having more than 260 words. A closer look at the 99.5th percentile shows that almost all summaries were copied from the beginning section of the article verbatim. In contrast, summaries with length between 134-183 words (99th to 99.5th percentile) shows mostly summarizations. Therefore, we removed any summaries longer than 183 words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# remove summaries 135 words or longer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "sns.histplot(data=df[df['summary_length']<200], x=\"summary_length\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for d in df[(df['summary_length'] > 134) & (df['summary_length'] < 183)]['summary']:\n",
    "#     print(d)\n",
    "#     print('***************')\n",
    "\n",
    "print(np.percentile(df['summary_length'], [98, 99, 99.5, 99.8, 99.9, 99.95, 99.99]))\n",
    "\n",
    "df = df[df['summary_length'] < 137]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Histogram shows an interesting peak at length = 100. It's possible that this dataset was scraped from digests that limits to displaying the first 100 words. This is confirmed by looking at the summaries with exactly 100 words. Many ended with the character ..., indicating a truncation. This type of summary are displaying the first 100 words verbatim, which is not our goal of summarization. Therefore, any summaries that ends with ... were excluded. This excludes 49057 records"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# remove summaries with [...] at the end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(df[df['summary'].str.contains('\\...$')]))\n",
    "\n",
    "df = df[~df['summary'].str.contains('\\...$')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# remove articles shorter than 100 words or longer than 1000 words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need some minimal length for articles to prevent there been too little information. Therefore, articles with fewer than 100 words were removed. This removed 47299 records. On the upper end, due to limitation of our model architecture, we don't want articles to be overly long."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['article_length'] = df['article'].apply(lambda x: len(x.split()))\n",
    "print(len(df[df['article_length']<100]))\n",
    "print(len(df[df['article_length']>1000]))\n",
    "\n",
    "df = df[(df['article_length']>= 100) & (df['article_length'] <= 1000)]\n",
    "\n",
    "print(len(df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To further weed out low quality summaries that simply copy the first few sentences of the article, we trimmed down the article to 150 words, and get rouge2 score for the trimmed summaries compared against the trimmed articles. Then we chose an arbitrary rouge-2 score threshold based on manual inspection of the score. An overly high score would imply the summary is simplying copying from the first sentence of the article. Those are removed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# remove any summaries with rouge2 score > 0.15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 0.15\n",
    "\n",
    "above_threshold = df[df['reference_rouge2'] > threshold]\n",
    "region_of_interest = df[(df['reference_rouge2'] > threshold - 0.05) & (df['reference_rouge2'] < threshold)]\n",
    "print(f'number of records to be removed: {len(above_threshold)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "row = random.randint(0, len(region_of_interest)-1)\n",
    "print(region_of_interest.iloc[row]['summary'])\n",
    "print('----------------------------')\n",
    "print(region_of_interest.iloc[row]['article'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df['reference_rouge2'] < threshold]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# remove articles and summaries with \"{\" in it, those indicate bad article parse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('before dropping badly parsed articles', len(df))\n",
    "df = df[~df['article'].str.contains('{')]\n",
    "print('after dropping badly parsed articles', len(df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# remove articles with more than 3 of the same 5-gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time\n",
    "\n",
    "t = time()\n",
    "\n",
    "def count_ngrams(text, n):\n",
    "    splitted = text.split(' ')\n",
    "    ngram_list = [splitted[i:i+n] for i in range(len(splitted)-n+1)]\n",
    "    ngram_string = [' '.join(x) for x in ngram_list]\n",
    "    return Counter(ngram_string)\n",
    "\n",
    "high_repeat = []\n",
    "for i, a in enumerate(df['article'].tolist()):\n",
    "    five_grams = count_ngrams(a, 5)\n",
    "    frequent_5grams = [(x, five_grams[x]) for x in five_grams if (five_grams[x] > 3) and (len(x.strip()) > 0)]\n",
    "    if frequent_5grams:\n",
    "        high_repeat.append(five_grams)\n",
    "    else:\n",
    "        high_repeat.append(None)\n",
    "\n",
    "print(time()-t)\n",
    "\n",
    "df['special'] = high_repeat\n",
    "\n",
    "print(len(df[df['special'].notna()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('before dropping similar articles', len(df))\n",
    "df = df[df['special'].isna()]\n",
    "print('after dropping similar articles', len(df))\n",
    "\n",
    "high_repeat = []\n",
    "for i, a in enumerate(df['summary'].tolist()):\n",
    "    trigrams = count_ngrams(a, 3)\n",
    "    frequent_3grams = [(x, trigrams[x]) for x in trigrams if (trigrams[x] > 3) and (len(x.strip()) > 0)]\n",
    "    if frequent_3grams:\n",
    "        high_repeat.append(trigrams)\n",
    "    else:\n",
    "        high_repeat.append(None)\n",
    "\n",
    "df['special'] = high_repeat\n",
    "\n",
    "print('number of similar summaries', len(df[df['special'].notna()]))\n",
    "\n",
    "df = df[df['special'].isna()]\n",
    "print('after dropping similar summary', len(df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other nonsense records found through manual spot checking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# USA today's topics does not contain any meaningful articles or summaries\n",
    "print('before dropping usatoday topics', len(df))\n",
    "df = df[df['category']!= 'topics']\n",
    "print('after dropping usatoday topics', len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# these blacklist words were found during manual checking that cannot be remvoed using all above filters\n",
    "# summaries containing these words are not meaningful\n",
    "df['special'] = df['article'].apply(lambda x: 'rent in SF for $4000+?\\n\\nImage' in x)\n",
    "df = df[~df['special']]\n",
    "df['special'] = df['article'].apply(lambda x: '5, 101, 4, 0) +' in x)\n",
    "df = df[~df['special']]\n",
    "\n",
    "print(len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# these blacklist words were found during manual checking that cannot be remvoed using all above filters\n",
    "# summaries containing these words are not meaningful\n",
    "blacklist = ['ARF ARF ARF', 'ceiling is lifted', 'Oscar predictions', 'FoxNews.com', 'Schuldercnn', 'usatoday']\n",
    "\n",
    "for phrase in blacklist:\n",
    "    print(phrase, len(df[df['summary'].str.contains(phrase)]))\n",
    "    df = df[~df['summary'].str.contains(phrase)]\n",
    "\n",
    "print('after dropping badly parsed summaries', len(df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[['original_row_idx', 'url', 'density_bin', 'article', 'summary', 'source', 'category']]\n",
    "\n",
    "print(len(df))\n",
    "\n",
    "df.to_csv('newsroom.csv', index = False)\n",
    "\n",
    "df = df.sample(frac = 1).reset_index(drop = True)\n",
    "\n",
    "train = df.iloc[:500000]\n",
    "test = df.iloc[500000:]\n",
    "\n",
    "train.to_csv('train.csv', index = False)\n",
    "test.to_csv('test.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
