{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json, gzip\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from time import time\n",
    "import re\n",
    "from numpy import nan as npnan\n",
    "import langid\n",
    "\n",
    "data = []\n",
    "\n",
    "with gzip.open(\"train.jsonl.gz\") as f:\n",
    "    for ln in f:\n",
    "        obj = json.loads(ln)\n",
    "        data.append(obj)\n",
    "        \n",
    "with gzip.open(\"dev.jsonl.gz\") as f:\n",
    "    for ln in f:\n",
    "        obj = json.loads(ln)\n",
    "        data.append(obj)\n",
    "\n",
    "# Test.jsonl.gz is not loaded because the summaries for those are randomly copied phrases throughout the article. \n",
    "# It was probably created as a challenge for leaderboards.\n",
    "\n",
    "# with gzip.open(\"test.jsonl.gz\") as f:\n",
    "#     for ln in f:\n",
    "#         obj = json.loads(ln)\n",
    "#         data.append(obj)\n",
    "\n",
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert the loaded data dictionary to pandas df\n",
    "url = [x['url'] for x in data]\n",
    "title = [x['title'] for x in data]\n",
    "date = [x['date'] for x in data]\n",
    "compression = [x['compression'] for x in data]\n",
    "coverage = [x['coverage'] for x in data]\n",
    "density = [x['density'] for x in data]\n",
    "compression_bin = [x['compression_bin'] for x in data]\n",
    "coverage_bin = [x['coverage_bin'] for x in data]\n",
    "density_bin = [x['density_bin'] for x in data]\n",
    "article = [x['text'] for x in data]\n",
    "summary = [x['summary'] for x in data]\n",
    "\n",
    "df = pd.DataFrame({'url': url,\n",
    "                   'date': date,\n",
    "                   'title': title,\n",
    "                   'compression': compression,\n",
    "                   'compression_bin': compression_bin,\n",
    "                   'coverage': coverage,\n",
    "                   'coverage_bin': coverage_bin,\n",
    "                   'density': density,\n",
    "                   'density_bin': density_bin,\n",
    "                   'article': article,\n",
    "                   'summary': summary})\n",
    "\n",
    "df = df.reset_index().rename(columns = {'index': 'original_row_idx'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate a rouge recall score for the first 150 words of the article\n",
    "# this will be used to filter out records where the reference summary is copying the beginning of the article verbatim.\n",
    "\n",
    "import rouge\n",
    "\n",
    "summary = df['summary'].apply(lambda x: ' '.join(str(x).split()[:150])).tolist()\n",
    "article = df['article'].apply(lambda x: ' '.join(str(x).split()[:150])).tolist()\n",
    "\n",
    "evaluator = rouge.Rouge(metrics=['rouge-n'],\n",
    "                           max_n=2,\n",
    "                           limit_length=True,\n",
    "                           length_limit=150,\n",
    "                           length_limit_type='words',\n",
    "                           apply_avg='Avg',\n",
    "                           apply_best='Best',\n",
    "                           alpha=0.5, # Default F1_score\n",
    "                           weight_factor=1.2,\n",
    "                           stemming=True)\n",
    "\n",
    "reference_rouge2 = []\n",
    "\n",
    "for i, pair in enumerate(zip(summary, article)):\n",
    "    if i%10000 == 0:\n",
    "        print(i)\n",
    "    reference_rouge2.append(evaluator.get_scores(pair[0], pair[1])['rouge-2']['r'])\n",
    "    \n",
    "df['reference_rouge2'] = reference_rouge2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parse out the source of the news from the URL. \n",
    "# Use UDF to process the splitted url since the root domain may not be in the same position after splitting due to subdomains\n",
    "def split_full_url(url):\n",
    "    splitted = url.split('/')\n",
    "    splitted += ['0']*9\n",
    "    char_only = [re.sub(r'[^A-z]','',x) for x in splitted[3:10]]\n",
    "    noempty = [splitted[2]]+[x if x not in ['', 'category'] else npnan for x in char_only]\n",
    "    return pd.Series(noempty)\n",
    "\n",
    "t = time()\n",
    "\n",
    "df[['u1', 'u2', 'u3', 'u4', 'u5', 'u6', 'u7', 'u8']] = df['url'].apply(split_full_url)\n",
    "\n",
    "print(time() - t)\n",
    "\n",
    "df['source'] = df['u1'].apply(lambda x: x.replace('.co.uk', '').replace('.com', '').replace('.go','')\\\n",
    "             .replace('.au', '').replace('.ca', '').replace(':9898','').replace('http:', '.foxnews').split('.')[-1])\n",
    "\n",
    "# these news sources have urls that may include the news categories. attempt to parse categories for these sources\n",
    "parseable = ['nytimes', 'theguardian', 'foxnews', 'usatoday', 'nydailynews', 'time', 'cnn', 'telegraph', \n",
    "             '9news', 'bostonglobe', 'latimes', 'bbc', 'sfgate', 'abcnews', 'nbcnews', 'nypost']\n",
    "\n",
    "df['category'] = df[['u2', 'u3', 'u4', 'u5', 'u6', 'u7', 'u8']].bfill(axis = 1).iloc[:, 0]\n",
    "df['category'] = df[['category', 'source']]\\\n",
    "    .apply(lambda x: x[0] if x[1] in parseable else 'unparsable', axis = 1).apply(lambda x: str(x).lower())\n",
    "\n",
    "df.drop(['u1', 'u2', 'u3', 'u4', 'u5', 'u6', 'u7', 'u8'], axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the first 150 words of the article to guess the language, in case it's not English\n",
    "import langid\n",
    "from time import time\n",
    "\n",
    "article = df['article'].apply(lambda x: ' '.join(str(x).split()[:150])).tolist()\n",
    "\n",
    "language = []\n",
    "\n",
    "t = time()\n",
    "for i, a in enumerate(article):\n",
    "    language.append(langid.classify(a)[0])\n",
    "    if i%10000 == 0:\n",
    "        print(i, time() - t)\n",
    "        \n",
    "df['language'] = language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('newsroom_training_data_original.csv', index = False)\n",
    "\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
