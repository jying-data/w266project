{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Generate Test Summaries.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2lCvePmEdE3t"
      },
      "source": [
        "### This notebook generates summaries for articles in test set using specified models, and calculates the ROUGE scores based on reference summary."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "STc0P92hQ9N-"
      },
      "source": [
        "!pip install py-rouge\n",
        "!pip install sentencepiece\n",
        "!pip install transformers\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8X1AV3VBdvgO"
      },
      "source": [
        "# model to use\n",
        "model_type = 'bart'\n",
        "\n",
        "# if model_path = None, load pretrained weights from Huggingface\n",
        "model_path = '/content/drive/MyDrive/NLP Summarization Project/Finetuned Models/HP optimized/'\n",
        "\n",
        "beam_size = 2\n",
        "batch_size = 10\n",
        "\n",
        "# path to the tokenized test articles\n",
        "test_tensor_path = '/content/drive/MyDrive/NLP Summarization Project/Data/Test_Articles_BART.pt'\n",
        "\n",
        "output_df_path = f'/content/drive/MyDrive/NLP Summarization Project/Generated Summaries/1f BART_finetuned on optimal hp_beam{beam_size}.csv'\n",
        "test_df_path = '/content/drive/MyDrive/NLP Summarization Project/Data/Cleaned Test Data.csv'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O5B_DNs3etf8"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from transformers import BartTokenizer, BartForConditionalGeneration\n",
        "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
        "from transformers import PegasusTokenizer, PegasusForConditionalGeneration\n",
        "from time import time\n",
        "import rouge\n",
        "from time import time\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "\n",
        "device = torch.device(\"cuda\")\n",
        "print('GPU:', torch.cuda.get_device_name(0))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y9qEtyYReul5"
      },
      "source": [
        "# load finetuned or pretrained weights\n",
        "\n",
        "if model_type.lower() == 'bart':\n",
        "    if not model_path:\n",
        "        model_path = \"facebook/bart-large-cnn\"\n",
        "    tokenizer = BartTokenizer.from_pretrained('facebook/bart-large-cnn')\n",
        "    model = BartForConditionalGeneration.from_pretrained(model_path)\n",
        "\n",
        "elif model_type.lower() == 't5':\n",
        "    if not model_path:\n",
        "        model_path = 't5-base'\n",
        "    tokenizer = T5Tokenizer.from_pretrained('t5-base')\n",
        "    model = T5ForConditionalGeneration.from_pretrained(model_path)\n",
        "\n",
        "elif model_type.lower() == 'pegasus':\n",
        "    model_path = 'google/pegasus-newsroom'\n",
        "    tokenizer = PegasusTokenizer.from_pretrained('google/pegasus-newsroom')\n",
        "    model = PegasusForConditionalGeneration.from_pretrained(model_path)\n",
        "\n",
        "model = model.to(device)\n",
        "\n",
        "articles_tokenized = torch.load(test_tensor_path)  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P65uqn5-e7Oo"
      },
      "source": [
        "# generate summaries\n",
        "\n",
        "start_time = time()\n",
        "\n",
        "def generate_summary(articles):\n",
        "    summary_list = []\n",
        "    num_of_articles = articles.size()[0]\n",
        "\n",
        "    for  start_row in range(0, num_of_articles, batch_size):\n",
        "        if (num_of_articles - start_row) < batch_size:\n",
        "            input_batch = articles[start_row:, :]\n",
        "        else:\n",
        "            input_batch = articles[start_row:start_row + batch_size, :]\n",
        "\n",
        "        input_batch = input_batch.to(device)\n",
        "\n",
        "        # specify pad_token_id instead of attention_mask because in the truncated conditions, the tokens are manually \n",
        "        # replaced using numpy, so the attention mask created during tokenization would not apply\n",
        "        summary_batch = model.generate(input_ids = input_batch, pad_token_id = tokenizer.pad_token_id , num_beams = beam_size)\n",
        "        summary_list += [tokenizer.decode(summary, skip_special_tokens = True, clean_up_tokenization_spaces=False) for summary in summary_batch]\n",
        "        if start_row%5000 == 0:\n",
        "            print(f'article {start_row}: time {round((time() - start_time)/60)}')\n",
        "            t = time()\n",
        "\n",
        "    return summary_list\n",
        "\n",
        "summary_list = generate_summary(articles_tokenized)\n",
        "\n",
        "print(round((time() - start_time)/3600))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A7cnJwYkf24X"
      },
      "source": [
        "test_df = pd.read_csv(test_df_path)\n",
        "print(len(test_df))\n",
        "\n",
        "# since the tokenized articles preserve the order on the test dataset csv, we can \n",
        "# simmply append the generated results as a new column\n",
        "test_df['generated'] = summary_list"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bD_iprnkRvmV"
      },
      "source": [
        "# evaluate the generated summary against the reference summary\n",
        "evaluator = rouge.Rouge(metrics=['rouge-n', 'rouge-l'],\n",
        "                           max_n=2,\n",
        "                           limit_length=True,\n",
        "                           length_limit=150,\n",
        "                           length_limit_type='words',\n",
        "                           apply_avg=False,\n",
        "                           apply_best=False,\n",
        "                           alpha=0.5, # Default F1_score\n",
        "                           weight_factor=1.2,\n",
        "                           stemming=True)\n",
        "\n",
        "t = time()\n",
        "\n",
        "scores = evaluator.get_scores(test_df[f'generated'], test_df['summary'])\n",
        "\n",
        "print( round(time() - t) )\n",
        "\n",
        "# saved the scores as separate columns in the test dataframe\n",
        "test_df['rouge1f'] = [x['f'][0] for x in scores['rouge-1']]\n",
        "test_df['rouge2f'] = [x['f'][0] for x in scores['rouge-2']]\n",
        "test_df['rougeLf'] = [x['f'][0] for x in scores['rouge-l']]\n",
        "test_df['rouge1r'] = [x['r'][0] for x in scores['rouge-1']]\n",
        "test_df['rouge2r'] = [x['r'][0] for x in scores['rouge-2']]\n",
        "test_df['rougeLr'] = [x['r'][0] for x in scores['rouge-l']]\n",
        "test_df['rouge1p'] = [x['p'][0] for x in scores['rouge-1']]\n",
        "test_df['rouge2p'] = [x['p'][0] for x in scores['rouge-2']]\n",
        "test_df['rougeLp'] = [x['p'][0] for x in scores['rouge-l']]\n",
        "\n",
        "test_df.to_csv(output_df_path)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}