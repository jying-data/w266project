# w266project

### W266 – Summer 2021 - Final Project
### Identifying Factors Impacting News Summarization Performance of Pre-trained Transformer Models
### Ali Nikooyan and Julia Ying

Instructions for the structure of this repository.
*	1 Data Loading and Cleaning : including scripts needed for downloading the Newsroom dataset from the source as well as subsequent EDA and data cleaning.
*	2 Tokenization: including scripts needed for tokenization of the articles with different default tokenizers  from Hugging Face (BART, PEGASUS, T5).
*	3 Train and Test: including scripts needed for model training, hyperparameter tuning, and testing of the model’s performance.
*	4 Result Analysis: including scripts needed for the analysis of the results and answering the research questions in this project.

The `.jsonl` files of the NEWSROOM raw data used in this study can be downloaded from [here](https://lil.nlp.cornell.edu/newsroom/)

The cleaned data in `.csv` and `.pytorch` format, along with saved finetuned BART model, can be viewed at this [Google Drive Link](https://drive.google.com/drive/folders/1raOB5P3LxFCVcAUIWcgacaLKZLwq0EE1)
